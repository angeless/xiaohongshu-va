#!/usr/bin/env python3
"""
é€‚é…æˆ‘ä»¬æ•°æ®åº“ç»“æ„çš„Notionæ¨é€è„šæœ¬
"""

import os
import sys
import json
import re
import glob
import ssl
import urllib.request
from datetime import datetime
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('.env.local')

API_KEY = os.getenv("NOTION_TOKEN", "")
CONTENT_DB = os.getenv("NOTION_DATABASE_ID", "")
WORK_DIR = "/home/angeless_wanganqi/.openclaw/workspace/video-copy-analyzer/workspace_data"

def make_request(method, endpoint, data=None):
    url = f"https://api.notion.com/v1{endpoint}"
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Notion-Version": "2022-06-28",
        "Content-Type": "application/json"
    }
    
    try:
        context = ssl._create_unverified_context()
        if data:
            req_data = json.dumps(data).encode('utf-8')
            request = urllib.request.Request(url, data=req_data, headers=headers, method=method)
        else:
            request = urllib.request.Request(url, headers=headers, method=method)
        
        with urllib.request.urlopen(request, context=context) as response:
            return json.loads(response.read().decode('utf-8'))
    except Exception as e:
        print(f"APIé”™è¯¯: {str(e)[:200]}")
        return None

def parse_number(text):
    if not text: return 0
    text = str(text).strip().lower()
    try:
        text = text.replace('+', '')
        if 'ä¸‡' in text: return int(float(text.replace('ä¸‡', '')) * 10000)
        elif 'w' in text: return int(float(text.replace('w', '')) * 10000)
        elif 'k' in text: return int(float(text.replace('k', '')) * 1000)
        else:
            clean_text = re.sub(r'[^\d.]', '', text)
            return int(float(clean_text)) if clean_text else 0
    except: return 0

def trim(t, l=1900): 
    return t[:l] + "..." if t and len(str(t)) > l else str(t)

def push_to_notion(meta, analysis_data):
    title = meta.get('title', 'æ— æ ‡é¢˜')
    print(f"ğŸ“ ä¸ŠæŠ¥: {title[:30]}...")
    
    analysis = analysis_data['analysis']
    transcript = analysis_data.get('transcript', '')
    duration = analysis_data.get('duration', '')
    
    # è§£ææ•°æ®
    likes = parse_number(meta['stats']['likes'])
    collects = parse_number(meta['stats']['collects'])
    comments = parse_number(meta['stats']['comments'])
    
    collect_ratio = round((collects / likes) * 100, 1) if likes > 0 else 0
    fire_mark = "ğŸ”¥" if collect_ratio > 50 else ""
    stats_str = f"ğŸ‘{likes} | â­ï¸{collects} ({collect_ratio}%){fire_mark} | ğŸ’¬{comments}"
    
    combined_insight = f"ã€äº‰åµç‚¹ã€‘ï¼š{analysis.get('comment_basic', 'æ— ')}\n\nã€æ·±åº¦æ´å¯Ÿã€‘ï¼š{analysis.get('hot_comment_deep', 'æ— ')}"
    
    # æ„å»ºpropertiesï¼ˆä½¿ç”¨æˆ‘ä»¬çš„å­—æ®µåï¼‰
    properties = {
        "ç¬”è®°æ ‡é¢˜": {"title": [{"text": {"content": title}}]},
        "ä½œè€…": {"rich_text": [{"text": {"content": meta.get('author', 'Unknown')}}]},
        "å‘å¸ƒæ—¶é—´": {"date": {"start": datetime.now().isoformat()}},
        "ç‚¹èµæ•°": {"number": likes},
        "æ”¶è—æ•°": {"number": collects},
        "è¯„è®ºæ•°": {"number": comments},
        "èµ": {"number": likes},
        "è—": {"number": collects},
        "è¯„": {"number": comments},
        "äº’åŠ¨æ•°æ®": {"rich_text": [{"text": {"content": stats_str}}]},
        "å£æ’­æ–‡æ¡ˆ": {"rich_text": [{"text": {"content": trim(transcript)}}]},
        "é¡µé¢æ–‡æ¡ˆ": {"rich_text": [{"text": {"content": trim(meta.get('desc', ''))}}]},
        "äº®ç‚¹æ€»ç»“": {"rich_text": [{"text": {"content": analysis.get('highlights', '')}}]},
        "èµ›é“": {"rich_text": [{"text": {"content": analysis.get('niche', '')}}]},
        "äººç¾¤æ ‡ç­¾": {"rich_text": [{"text": {"content": analysis.get('target_audience', '')}}]},
        "å‰ä¸‰ç§’é’©å­": {"rich_text": [{"text": {"content": analysis.get('hook_3s', '')}}]},
        "é‡‘å¥é’©å­": {"rich_text": [{"text": {"content": analysis.get('golden_sentence', '')}}]},
        "æ ¸å¿ƒç»“æ„": {"rich_text": [{"text": {"content": analysis.get('structure', '')}}]},
        "ç»“æ„åŒ–åˆ†æ": {"rich_text": [{"text": {"content": analysis.get('structure', '')}}]},
        "æƒ…ç»ªèµ°å‘": {"rich_text": [{"text": {"content": analysis.get('emotion_arc', '')}}]},
        "æ‹æ‘„å½¢å¼": {"rich_text": [{"text": {"content": analysis.get('visual_form', '')}}]},
        "ä¸å¯æ›¿ä»£æ€§": {"rich_text": [{"text": {"content": analysis.get('why_him', '')}}]},
        "çƒ­è¯„åˆ†æ": {"rich_text": [{"text": {"content": trim(combined_insight)}}]},
        "çƒ­è¯„æ´å¯Ÿ": {"rich_text": [{"text": {"content": trim(combined_insight)}}]},
        "å¹³å°ä¿¡å·": {"rich_text": [{"text": {"content": analysis.get('platform_signal', '')}}]},
        "æ•ˆæœæ‰“åˆ†": {"rich_text": [{"text": {"content": analysis.get('score_breakdown', '')}}]},
        "è¯„çº§": {"rich_text": [{"text": {"content": analysis.get('grade', '')}}]},
        "é€šç”¨å…¬å¼": {"rich_text": [{"text": {"content": analysis.get('universal_formula', '')}}]},
        "æˆ‘çš„é€‰é¢˜": {"rich_text": [{"text": {"content": analysis.get('my_new_topics', '')}}]},
        "é€‰é¢˜é‡‘çŸ¿": {"rich_text": [{"text": {"content": analysis.get('my_new_topics', '')}}]},
        "çµæ„Ÿå¯ç¤º": {"rich_text": [{"text": {"content": analysis.get('my_new_topics', '')}}]},
        "æ‹’ç»æ–¹å‘": {"rich_text": [{"text": {"content": analysis.get('refuse_direction', '')}}]},
        "å¯æŠ„ä½œä¸š": {"rich_text": [{"text": {"content": analysis.get('can_copy', '')}}]},
        "é¿å‘æŒ‡å—": {"rich_text": [{"text": {"content": analysis.get('cannot_copy', '')}}]},
        "ä¸€å¥å¤ç”¨": {"rich_text": [{"text": {"content": analysis.get('reusable_sentence', '')}}]},
        "ç¬”è®°ç±»å‹": {"select": {"name": "è§†é¢‘"}},
        "å†…å®¹åˆ†ç±»": {"select": {"name": "å¹²è´§"}},
    }
    
    # åˆ›å»ºé¡µé¢
    data = {
        "parent": {"database_id": CONTENT_DB},
        "properties": properties
    }
    
    result = make_request("POST", "/pages", data)
    if result:
        page_id = result.get("id")
        print(f"  âœ… Notionæ¨é€æˆåŠŸ")
        print(f"     é¡µé¢: https://www.notion.so/{page_id.replace('-', '')}")
        return page_id
    else:
        print(f"  âŒ Notionæ¨é€å¤±è´¥")
        return None

def main():
    print("=" * 60)
    print("æ¨é€åˆ°Notion - é€‚é…ç‰ˆ")
    print("=" * 60)
    
    # æ‰¾åˆ°æ‰€æœ‰åˆ†ææŠ¥å‘Š
    analysis_files = glob.glob(os.path.join(WORK_DIR, "analysis_*.json"))
    
    if not analysis_files:
        print("âŒ æœªæ‰¾åˆ°åˆ†ææŠ¥å‘Š")
        return
    
    print(f"ğŸ“‹ å‘ç° {len(analysis_files)} ä»½æŠ¥å‘Š\n")
    
    for i, analysis_path in enumerate(analysis_files, 1):
        try:
            with open(analysis_path, 'r', encoding='utf-8') as f:
                analysis_data = json.load(f)
            
            # æ‰¾åˆ°å¯¹åº”çš„metaæ–‡ä»¶
            meta_path = analysis_data.get('meta_file_path')
            if not meta_path or not os.path.exists(meta_path):
                # å°è¯•æ¨æ–­metaæ–‡ä»¶è·¯å¾„
                base_name = os.path.basename(analysis_path).replace("analysis_", "meta_")
                meta_path = os.path.join(os.path.dirname(analysis_path), base_name)
            
            if not os.path.exists(meta_path):
                print(f"âš ï¸ æ‰¾ä¸åˆ°Metaæ–‡ä»¶ï¼Œè·³è¿‡: {analysis_path}")
                continue
            
            with open(meta_path, 'r', encoding='utf-8') as f:
                meta = json.load(f)
            
            print(f"ğŸ“¤ [{i}/{len(analysis_files)}] {meta.get('title', 'æ— æ ‡é¢˜')[:30]}...")
            push_to_notion(meta, analysis_data)
            
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
        
        time.sleep(0.5)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ¨é€å®Œæˆï¼")
    print("=" * 60)

if __name__ == "__main__":
    import time
    main()
