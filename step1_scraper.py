import os
import sys
import json
import time
import re
import random
import subprocess
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from urllib.parse import urljoin
from datetime import datetime
from playwright.sync_api import sync_playwright

# ç¡®ä¿å·¥ä½œç›®å½•å­˜åœ¨
WORK_DIR = "workspace_data"
os.makedirs(WORK_DIR, exist_ok=True)
USER_DATA_DIR = "./browser_memory"

LOGIN_PAGE_SELECTORS = [
    '.login-container',
    '.login-mask',
    'input[placeholder*="æ‰‹æœºå·"]',
    'input[placeholder*="éªŒè¯ç "]',
]

LOGIN_HINT_WORDS = [
    "æ‰«ç ç™»å½•",
    "ç™»å½•å",
    "è¯·å…ˆç™»å½•",
    "æ‰‹æœºå·ç™»å½•",
    "éªŒè¯ç ç™»å½•",
]

PROFILE_URL_HINTS = ["/user/profile/", "www.xiaohongshu.com/user/"]
LOGIN_COOKIE_PREFIXES = ("web_session",)
DEFAULT_LOGIN_WAIT_SECONDS = int(os.getenv("LOGIN_WAIT_SECONDS", "300"))
STRICT_LOGIN_REQUIRED = os.getenv("STRICT_LOGIN_REQUIRED", "1") != "0"
LOGIN_SUCCESS_SELECTORS = [
    '[href*="/user/profile"]',
    '.user-side-bar',
    '.author-wrapper',
]

def get_robust_session():
    session = requests.Session()
    retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

def download_video(url, filename):
    if not url or url.startswith("blob:"):
        print(f"âŒ æ— æ³•ä¸‹è½½ï¼ŒURLæ— æ•ˆæˆ–ä¸ºBlob: {url}")
        return None
        
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.xiaohongshu.com/"
        }
        print(f"ğŸ“¥ æ­£åœ¨è¯·æ±‚è§†é¢‘æµ (å«é‡è¯•æœºåˆ¶): {url[:50]}...")
        
        session = get_robust_session()
        response = session.get(url, headers=headers, stream=True, timeout=120)
        
        if response.status_code == 200:
            save_path = os.path.join(WORK_DIR, filename)
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=1024*1024):
                    if chunk: f.write(chunk)
            
            if os.path.getsize(save_path) < 1024:
                print("âš ï¸ ä¸‹è½½æ–‡ä»¶è¿‡å°ï¼Œå¯èƒ½å·²æŸå")
                return None
                
            print(f"âœ… è§†é¢‘ä¸‹è½½å®Œæˆ: {save_path}")
            return save_path
        else:
            print(f"âš ï¸ ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
    except Exception as e:
        print(f"âš ï¸ ä¸‹è½½æµå‡ºé”™: {e}")
    return None


def download_video_with_ytdlp(note_url, timestamp):
    """å…œåº•ä¸‹è½½ï¼šç›´æ¥ä½¿ç”¨ yt-dlp ä¸‹è½½ç¬”è®°è§†é¢‘ã€‚"""
    template = os.path.join(WORK_DIR, f"video_{timestamp}.%(ext)s")
    cmd = [
        "yt-dlp",
        "-f",
        "bv*[height<=1080]+ba/b[height<=1080]/b",
        "--merge-output-format",
        "mp4",
        "-o",
        template,
        note_url,
    ]
    try:
        print(f"ğŸ›Ÿ [Fallback] ä½¿ç”¨ yt-dlp å…œåº•ä¸‹è½½: {note_url[:80]}...")
        proc = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
        if proc.returncode != 0:
            msg = (proc.stderr or proc.stdout or "").strip()
            print(f"âš ï¸ yt-dlp å…œåº•ä¸‹è½½å¤±è´¥: {msg[:300]}")
            return None
        # ä¼˜å…ˆ mp4ï¼Œå…¶æ¬¡åŒ¹é…åŒå‰ç¼€æ–‡ä»¶
        mp4_path = os.path.join(WORK_DIR, f"video_{timestamp}.mp4")
        if os.path.exists(mp4_path):
            return mp4_path
        prefix = os.path.join(WORK_DIR, f"video_{timestamp}.")
        candidates = [
            os.path.join(WORK_DIR, f)
            for f in os.listdir(WORK_DIR)
            if f.startswith(f"video_{timestamp}.")
        ]
        return candidates[0] if candidates else None
    except Exception as e:
        print(f"âš ï¸ yt-dlp å…œåº•ä¸‹è½½å¼‚å¸¸: {e}")
        return None

def _has_login_cookie(context):
    if not context:
        return False
    try:
        cookies = context.cookies("https://www.xiaohongshu.com")
    except Exception:
        try:
            cookies = context.cookies()
        except Exception:
            cookies = []
    names = {str(c.get("name", "")).lower() for c in cookies if isinstance(c, dict)}
    return any(any(name.startswith(prefix) for prefix in LOGIN_COOKIE_PREFIXES) for name in names)


def _has_note_content(page):
    try:
        if page.query_selector("video"):
            return True
    except Exception:
        pass
    try:
        if page.query_selector("#detail-desc"):
            return True
    except Exception:
        pass
    return False


def _has_login_success_marker(page):
    for selector in LOGIN_SUCCESS_SELECTORS:
        try:
            if page.query_selector(selector):
                return True
        except Exception:
            continue
    return False


def page_requires_login(page, context=None):
    """åˆ¤æ–­é¡µé¢æ˜¯å¦å¤„äºç™»å½•æ€ç¼ºå¤±åœºæ™¯ã€‚"""
    try:
        if page.is_closed():
            return False
    except Exception:
        return False

    signal_score = 0
    try:
        url = (page.url or "").lower()
        if "/login" in url:
            signal_score += 2
    except Exception:
        pass

    has_login_selector = False
    for selector in LOGIN_PAGE_SELECTORS:
        try:
            if page.query_selector(selector):
                has_login_selector = True
                break
        except Exception:
            continue

    has_login_hint = False
    try:
        content = page.content()
        if any(word in content for word in LOGIN_HINT_WORDS):
            has_login_hint = True
    except Exception:
        pass

    if has_login_selector:
        signal_score += 1
    if has_login_hint:
        signal_score += 1
    if signal_score >= 2 and not _has_login_success_marker(page):
        return True

    # æ—¢æ²¡æœ‰æ˜ç¡®ç™»å½•é¡µä¿¡å·ï¼Œåˆæœ‰ç™»å½•æˆåŠŸ/å†…å®¹ä¿¡å·æ—¶ï¼Œä¸è§†ä¸ºéœ€è¦ç™»å½•
    if _has_login_success_marker(page) or _has_note_content(page):
        return False

    # æ— æ˜æ˜¾ä¿¡å·æ—¶äº¤ç»™ä¸¥æ ¼æ¨¡å¼ç­–ç•¥ï¼ˆåœ¨ wait_for_login_if_needed é‡Œå¤„ç†ï¼‰
    return False


def wait_for_login_if_needed(page, context=None, timeout_seconds=None, poll_seconds=2, force_wait=False):
    """è‹¥æ£€æµ‹åˆ°ç™»å½•é¡µé¢ï¼Œåˆ™ç­‰å¾…ç”¨æˆ·ç™»å½•å®Œæˆåç»§ç»­ã€‚"""
    if timeout_seconds is None:
        timeout_seconds = DEFAULT_LOGIN_WAIT_SECONDS

    try:
        if page.is_closed():
            print("âš ï¸ é¡µé¢å·²å…³é—­ï¼Œè·³è¿‡ç™»å½•ç­‰å¾…ã€‚")
            return False
    except Exception:
        return False

    has_cookie = _has_login_cookie(context)
    requires_login = page_requires_login(page, context=context)

    # é»˜è®¤ä¸¥æ ¼æ¨¡å¼ï¼šæ²¡æœ‰ç™»å½•æ€å°±å…ˆç­‰å¾…ï¼Œé¿å…é¡µé¢åˆšå¼€å°±å…³é—­ï¼Œç”¨æˆ·æ¥ä¸åŠæ‰«ç ã€‚
    if force_wait:
        print(
            f"ğŸ” å°†å¼ºåˆ¶ç­‰å¾…ç™»å½•ï¼ˆæœ€å¤š {timeout_seconds} ç§’ï¼‰ï¼Œè¯·æ‰«ç å®Œæˆåç»§ç»­..."
        )
    elif not has_cookie and STRICT_LOGIN_REQUIRED:
        print(
            f"ğŸ” æœªæ£€æµ‹åˆ°ç™»å½•æ€ï¼ˆweb_sessionï¼‰ï¼Œå°†ç­‰å¾…æœ€å¤š {timeout_seconds} ç§’ä¾›ä½ æ‰«ç ç™»å½•..."
        )
    elif not requires_login:
        return True
    else:
        print("ğŸ” æ£€æµ‹åˆ°éœ€è¦ç™»å½•ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­æ‰«ç å®Œæˆç™»å½•ï¼Œè„šæœ¬å°†è‡ªåŠ¨ç»§ç»­...")

    deadline = time.time() + timeout_seconds
    while time.time() < deadline:
        try:
            if page.is_closed():
                print("âš ï¸ é¡µé¢å·²å…³é—­ï¼Œç»“æŸç™»å½•ç­‰å¾…ã€‚")
                return False
        except Exception:
            return False

        has_cookie = _has_login_cookie(context)
        requires_login = page_requires_login(page, context=context)

        # ä¸¥æ ¼æ¨¡å¼ä¸‹ï¼Œä¼˜å…ˆç­‰åˆ°å¯ç”¨ç™»å½•æ€ï¼›éä¸¥æ ¼æ¨¡å¼æŒ‰é¡µé¢çŠ¶æ€æ”¾è¡Œã€‚
        if has_cookie and (not requires_login or _has_note_content(page) or _has_login_success_marker(page)):
            print("âœ… æ£€æµ‹åˆ°ç™»å½•å®Œæˆï¼Œç»§ç»­æ‰§è¡ŒæŠ“å–ã€‚")
            try:
                page.wait_for_load_state("domcontentloaded", timeout=5000)
            except Exception:
                pass
            return True

        if not force_wait and not STRICT_LOGIN_REQUIRED and not requires_login:
            return True

        try:
            page.wait_for_timeout(int(poll_seconds * 1000))
        except KeyboardInterrupt:
            print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç™»å½•ç­‰å¾…ã€‚")
            return False
        except Exception:
            try:
                if page.is_closed():
                    print("âš ï¸ é¡µé¢/æµè§ˆå™¨å·²å…³é—­ï¼Œç»“æŸç™»å½•ç­‰å¾…ã€‚")
                    return False
            except Exception:
                return False
            time.sleep(poll_seconds)

    if STRICT_LOGIN_REQUIRED and not _has_login_cookie(context):
        print("âš ï¸ ç™»å½•ç­‰å¾…è¶…æ—¶ï¼Œä»æœªæ£€æµ‹åˆ°ç™»å½•æ€ã€‚")
    else:
        print("âš ï¸ ç™»å½•ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­å°è¯•æŠ“å–ï¼ˆå¯èƒ½å¤±è´¥ï¼‰ã€‚")
    return False


def is_profile_url(url):
    if not url:
        return False
    u = str(url).lower()
    return any(hint in u for hint in PROFILE_URL_HINTS) and "/explore/" not in u


def _extract_note_id(url):
    if not url:
        return "unknown"
    match = re.search(r"/explore/(\w+)", url)
    return match.group(1) if match else "unknown"


def _extract_stats_from_page(page):
    stats = {'likes': '0', 'collects': '0', 'comments': '0'}
    try:
        counts = page.query_selector_all(".interact-container .count")
        if len(counts) >= 3:
            stats['likes'] = counts[0].inner_text()
            stats['collects'] = counts[1].inner_text()
            stats['comments'] = counts[2].inner_text()
        else:
            content = page.content()
            likes_match = re.search(r'(?:ç‚¹èµ|èµ)\s*([\d\.wä¸‡k]+)', content)
            collects_match = re.search(r'(?:æ”¶è—|è—)\s*([\d\.wä¸‡k]+)', content)
            comments_match = re.search(r'(?:è¯„è®º|è¯„)\s*([\d\.wä¸‡k]+)', content)
            if likes_match:
                stats['likes'] = likes_match.group(1)
            if collects_match:
                stats['collects'] = collects_match.group(1)
            if comments_match:
                stats['comments'] = comments_match.group(1)
    except Exception as e:
        print(f"âš ï¸ æŠ“å–äº’åŠ¨æ•°æ®å¾®ç‘•: {e}")
    return stats


def _resolve_video_url(page, sniffed_url=None):
    final_download_url = sniffed_url
    if not final_download_url:
        try:
            content = page.content()
            matches = re.findall(r'"masterUrl":"(http[^"]+)"', content)
            if matches:
                final_download_url = matches[0].encode('utf-8').decode('unicode_escape')
                print(f"ğŸ” æºç æå–æˆåŠŸ: {final_download_url[:40]}...")
            if not final_download_url:
                # å…œåº•æå–é¡µé¢ä¸­ç›´å‡ºçš„ mp4 èµ„æº
                mp4_matches = re.findall(r'(https?://[^"\\]+?\.mp4[^"\\]*)', content)
                if mp4_matches:
                    final_download_url = mp4_matches[0]
                    print(f"ğŸ” æºç å…œåº•æå– mp4 æˆåŠŸ: {final_download_url[:40]}...")
        except Exception as e:
            print(f"âš ï¸ æºç æå–å¤±è´¥: {e}")
    if not final_download_url:
        try:
            video_element = page.query_selector('video')
            if video_element:
                src = video_element.get_attribute("src")
                if src and not src.startswith("blob:"):
                    final_download_url = src
        except Exception:
            pass
    return final_download_url


def _extract_note_meta(page, source_url, sniffed_video_url=None):
    note_url = page.url if page.url else source_url
    note_id = _extract_note_id(note_url)
    timestamp = int(datetime.now().timestamp() * 1000)

    print("â³ ç¼“å†² 3 ç§’ä»¥ç¡®ä¿äº’åŠ¨æ•°æ®åŠ è½½...")
    time.sleep(3)
    try:
        page.mouse.wheel(0, 500)
    except Exception:
        pass
    time.sleep(1)

    stats = _extract_stats_from_page(page)
    print(f"ğŸ“Š æŠ“å–åˆ°æ•°æ®ï¼šèµ({stats['likes']}) è—({stats['collects']}) è¯„({stats['comments']})")

    final_download_url = _resolve_video_url(page, sniffed_video_url)
    if not final_download_url:
        raise Exception("âŒ æœªèƒ½æ‰¾åˆ°æœ‰æ•ˆçš„è§†é¢‘åœ°å€")

    title = page.title()
    desc = ""
    try:
        desc_element = page.query_selector("#detail-desc")
        if desc_element:
            desc = desc_element.inner_text()
    except Exception:
        pass

    author = "Unknown"
    try:
        author_elem = page.query_selector(".username")
        if author_elem:
            author = author_elem.inner_text().strip()
    except Exception:
        pass
    author = author.replace("å…³æ³¨", "").strip()

    comments_list = []
    try:
        comment_elements = page.query_selector_all(".comment-item .content")
        for el in comment_elements[:5]:
            comments_list.append(el.inner_text())
    except Exception:
        pass

    cover_url = ""
    try:
        meta_img = page.query_selector('meta[property="og:image"]')
        if meta_img:
            cover_url = meta_img.get_attribute("content")
    except Exception:
        pass

    video_filename = f"video_{timestamp}.mp4"
    local_video_path = None
    if final_download_url:
        print(f"ğŸ“¥ [Video] å‡†å¤‡ä¸‹è½½...")
        local_video_path = download_video(final_download_url, video_filename)

    # é¡µé¢ææµå¤±è´¥æ—¶ï¼Œä½¿ç”¨ yt-dlp å…œåº•
    if not local_video_path:
        local_video_path = download_video_with_ytdlp(note_url or source_url, timestamp)

    if not local_video_path:
        raise Exception("âŒ æœªèƒ½æ‰¾åˆ°æœ‰æ•ˆçš„è§†é¢‘åœ°å€")

    meta_data = {
        "id": note_id,
        "url": note_url or source_url,
        "title": title,
        "author": author,
        "desc": desc,
        "stats": stats,
        "top_comments": "\n".join(comments_list),
        "cover_url": cover_url,
        "local_video_path": local_video_path,
        "timestamp": timestamp
    }

    json_filename = f"meta_{timestamp}.json"
    json_path = os.path.join(WORK_DIR, json_filename)
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(meta_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… å…ƒæ•°æ®ä¿å­˜å®Œæˆ: {json_filename}")
    return json_path


def _collect_note_links(page):
    links = []
    try:
        anchors = page.query_selector_all('a[href*="/explore/"]')
        for a in anchors:
            href = a.get_attribute("href")
            if not href:
                continue
            full = urljoin("https://www.xiaohongshu.com", href)
            if "/explore/" in full:
                links.append(full)
    except Exception:
        pass
    # ä¿åºå»é‡
    seen = set()
    uniq = []
    for u in links:
        if u not in seen:
            seen.add(u)
            uniq.append(u)
    return uniq


def _click_note_card(page, note_url):
    """ä¼˜å…ˆæ¨¡æ‹ŸçœŸå®ç‚¹å‡»å¡ç‰‡è¿›å…¥è¯¦æƒ…é¡µã€‚"""
    try:
        anchors = page.query_selector_all('a[href*="/explore/"]')
        for a in anchors:
            href = a.get_attribute("href")
            if not href:
                continue
            full = urljoin("https://www.xiaohongshu.com", href)
            if full != note_url:
                continue
            try:
                a.scroll_into_view_if_needed(timeout=3000)
            except Exception:
                pass
            a.click(timeout=6000)
            return True
    except Exception:
        pass
    return False

def run_scraper(url):
    print(f"ğŸš€ [Step 1] å¯åŠ¨çŒäººæ¨¡å¼: {url}")

    with sync_playwright() as p:
        context = None
        if not os.path.exists(USER_DATA_DIR):
             print(f"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°æµè§ˆå™¨è®°å¿†æ–‡ä»¶å¤¹ï¼Œè¯·å…ˆè¿è¡Œ login_tool.pyï¼")

        print(f"ğŸ‘€ æ­£åœ¨å”¤é†’æœ‰è®°å¿†çš„æµè§ˆå™¨...")
        try:
            context = p.chromium.launch_persistent_context(
                user_data_dir=USER_DATA_DIR,
                headless=False,
                viewport={'width': 1280, 'height': 800},
                channel="chrome", 
                args=['--no-sandbox', '--disable-blink-features=AutomationControlled']
            )
        except:
            context = p.chromium.launch_persistent_context(
                user_data_dir=USER_DATA_DIR,
                headless=False,
                viewport={'width': 1280, 'height': 800},
                args=['--no-sandbox', '--disable-blink-features=AutomationControlled']
            )

        try:
            if len(context.pages) > 0:
                page = context.pages[0]
            else:
                page = context.new_page()
            
            real_video_url = {"url": None}
            
            def handle_response(response):
                try:
                    if "video/mp4" in response.headers.get("content-type", "") or ".mp4" in response.url:
                        if "sns-video" in response.url or "spectrum" in response.url:
                            if not real_video_url["url"]: 
                                print(f"ğŸ•µï¸ å—…æ¢åˆ°çœŸå®è§†é¢‘æµ: {response.url[:40]}...")
                                real_video_url["url"] = response.url
                except Exception:
                    pass

            page.on("response", handle_response)
            page.add_init_script("Object.defineProperty(navigator, 'webdriver', { get: () => undefined })")

            try:
                print("ğŸŒ æ­£åœ¨åŠ è½½é¡µé¢ (è®¾ç½®30ç§’è¶…æ—¶)...")
                page.goto(url, wait_until="domcontentloaded", timeout=30000)
            except Exception as e:
                print(f"âš ï¸ é¡µé¢åŠ è½½æç¤º (Timeout)ï¼Œæ­£åœ¨åœæ­¢ç½‘é¡µè½¬åœˆä»¥æå–æ•°æ®...")
                try:
                    page.evaluate("window.stop()")
                except Exception:
                    pass

            wait_for_login_if_needed(page, context=context)
            json_path = _extract_note_meta(page, url, real_video_url["url"])
            print("âœ… [Step 1 å®Œæˆ] æ•°æ®å·²ä¿å­˜")
            time.sleep(1)
            return json_path
        finally:
            if context:
                try:
                    context.close()
                except:
                    pass


def run_profile_scraper(profile_url, max_items=10):
    """è¾¾äººä¸»é¡µçœŸå®ç”¨æˆ·æ¨¡å¼ï¼šç‚¹å‡»å¡ç‰‡ -> åˆ†æ -> è¿”å› -> ä¸‹ä¸€æ¡ã€‚"""
    print(f"ğŸš€ [Step 1] è¾¾äººä¸»é¡µæ¨¡å¼: {profile_url}")
    print(f"ğŸ¯ ç›®æ ‡é‡‡é›†æ¡æ•°: {max_items}")

    with sync_playwright() as p:
        context = None
        results = []
        visited = set()
        try:
            try:
                context = p.chromium.launch_persistent_context(
                    user_data_dir=USER_DATA_DIR,
                    headless=False,
                    viewport={'width': 1280, 'height': 800},
                    channel="chrome",
                    args=['--no-sandbox', '--disable-blink-features=AutomationControlled']
                )
            except Exception:
                context = p.chromium.launch_persistent_context(
                    user_data_dir=USER_DATA_DIR,
                    headless=False,
                    viewport={'width': 1280, 'height': 800},
                    args=['--no-sandbox', '--disable-blink-features=AutomationControlled']
                )

            page = context.pages[0] if context.pages else context.new_page()
            page.add_init_script("Object.defineProperty(navigator, 'webdriver', { get: () => undefined })")

            print("ğŸŒ æ‰“å¼€è¾¾äººä¸»é¡µ...")
            page.goto(profile_url, wait_until="domcontentloaded", timeout=30000)
            wait_for_login_if_needed(page, context=context)
            time.sleep(2)

            idle_rounds = 0
            while len(results) < max_items and idle_rounds < 8:
                note_links = _collect_note_links(page)
                pending = [u for u in note_links if u not in visited]

                if not pending:
                    idle_rounds += 1
                    print("â†˜ï¸ æœªå‘ç°æ–°å¡ç‰‡ï¼Œå‘ä¸‹æ»šåŠ¨åŠ è½½æ›´å¤š...")
                    try:
                        page.mouse.wheel(0, 1800)
                    except Exception:
                        pass
                    time.sleep(2)
                    continue

                idle_rounds = 0
                for note_url in pending:
                    if len(results) >= max_items:
                        break
                    visited.add(note_url)
                    print(f"\nğŸ¬ è¿›å…¥ç¬”è®° ({len(results)+1}/{max_items}): {note_url}")

                    sniffed = {"url": None}

                    def handle_response(response):
                        try:
                            if "video/mp4" in response.headers.get("content-type", "") or ".mp4" in response.url:
                                if "sns-video" in response.url or "spectrum" in response.url:
                                    if not sniffed["url"]:
                                        sniffed["url"] = response.url
                        except Exception:
                            pass

                    page.on("response", handle_response)
                    try:
                        clicked = _click_note_card(page, note_url)
                        if clicked:
                            print("ğŸ–±ï¸ å·²æ¨¡æ‹Ÿç‚¹å‡»å¡ç‰‡è¿›å…¥è¯¦æƒ…é¡µã€‚")
                            time.sleep(2)
                        else:
                            print("âš ï¸ å¡ç‰‡ç‚¹å‡»å¤±è´¥ï¼Œé™çº§ä¸ºåŒä¼šè¯ç›´è¾¾è¯¦æƒ…é¡µã€‚")
                            try:
                                page.goto(note_url, wait_until="domcontentloaded", timeout=30000)
                            except Exception:
                                page.goto(note_url, wait_until="load", timeout=45000)

                        wait_for_login_if_needed(page, context=context)
                        json_path = _extract_note_meta(page, note_url, sniffed["url"])
                        results.append(json_path)
                        print(f"âœ… å½“å‰ç¬”è®°é‡‡é›†å®Œæˆ: {os.path.basename(json_path)}")
                    except Exception as e:
                        print(f"âŒ å½“å‰ç¬”è®°é‡‡é›†å¤±è´¥: {e}")
                    finally:
                        try:
                            page.remove_listener("response", handle_response)
                        except Exception:
                            pass

                    # æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¿”å›è¾¾äººä¸»é¡µ
                    try:
                        page.go_back(wait_until="domcontentloaded", timeout=15000)
                    except Exception:
                        page.goto(profile_url, wait_until="domcontentloaded", timeout=30000)
                    time.sleep(random.uniform(1.0, 2.2))

            print(f"\nğŸ‰ è¾¾äººä¸»é¡µé‡‡é›†ç»“æŸï¼ŒæˆåŠŸ {len(results)} æ¡ã€‚")
            return results
        finally:
            if context:
                try:
                    context.close()
                except Exception:
                    pass
