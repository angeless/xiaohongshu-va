import os
import sys
import json
import time
import re
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from datetime import datetime
from playwright.sync_api import sync_playwright

# ç¡®ä¿å·¥ä½œç›®å½•å­˜åœ¨
WORK_DIR = "workspace_data"
os.makedirs(WORK_DIR, exist_ok=True)
USER_DATA_DIR = "./browser_memory"

def get_robust_session():
    session = requests.Session()
    retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

def download_video(url, filename):
    if not url or url.startswith("blob:"):
        print(f"âŒ æ— æ³•ä¸‹è½½ï¼ŒURLæ— æ•ˆæˆ–ä¸ºBlob: {url}")
        return None
        
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.xiaohongshu.com/"
        }
        print(f"ğŸ“¥ æ­£åœ¨è¯·æ±‚è§†é¢‘æµ (å«é‡è¯•æœºåˆ¶): {url[:50]}...")
        
        session = get_robust_session()
        response = session.get(url, headers=headers, stream=True, timeout=120)
        
        if response.status_code == 200:
            save_path = os.path.join(WORK_DIR, filename)
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=1024*1024):
                    if chunk: f.write(chunk)
            
            if os.path.getsize(save_path) < 1024:
                print("âš ï¸ ä¸‹è½½æ–‡ä»¶è¿‡å°ï¼Œå¯èƒ½å·²æŸå")
                return None
                
            print(f"âœ… è§†é¢‘ä¸‹è½½å®Œæˆ: {save_path}")
            return save_path
        else:
            print(f"âš ï¸ ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
    except Exception as e:
        print(f"âš ï¸ ä¸‹è½½æµå‡ºé”™: {e}")
    return None

def run_scraper(url):
    print(f"ğŸš€ [Step 1] å¯åŠ¨çŒäººæ¨¡å¼: {url}")
    
    note_id = "unknown"
    match = re.search(r'/explore/(\w+)', url)
    if match:
        note_id = match.group(1)
    
    timestamp = int(datetime.now().timestamp())
    
    with sync_playwright() as p:
        context = None
        if not os.path.exists(USER_DATA_DIR):
             print(f"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°æµè§ˆå™¨è®°å¿†æ–‡ä»¶å¤¹ï¼Œè¯·å…ˆè¿è¡Œ login_tool.pyï¼")

        print(f"ğŸ‘€ æ­£åœ¨å”¤é†’æœ‰è®°å¿†çš„æµè§ˆå™¨...")
        try:
            context = p.chromium.launch_persistent_context(
                user_data_dir=USER_DATA_DIR,
                headless=False,
                viewport={'width': 1280, 'height': 800},
                channel="chrome", 
                args=['--no-sandbox', '--disable-blink-features=AutomationControlled']
            )
        except:
            context = p.chromium.launch_persistent_context(
                user_data_dir=USER_DATA_DIR,
                headless=False,
                viewport={'width': 1280, 'height': 800},
                args=['--no-sandbox', '--disable-blink-features=AutomationControlled']
            )

        try:
            if len(context.pages) > 0:
                page = context.pages[0]
            else:
                page = context.new_page()
            
            real_video_url = {"url": None}
            
            def handle_response(response):
                try:
                    if "video/mp4" in response.headers.get("content-type", "") or ".mp4" in response.url:
                        if "sns-video" in response.url or "spectrum" in response.url:
                            if not real_video_url["url"]: 
                                print(f"ğŸ•µï¸ å—…æ¢åˆ°çœŸå®è§†é¢‘æµ: {response.url[:40]}...")
                                real_video_url["url"] = response.url
                except: pass

            page.on("response", handle_response)
            page.add_init_script("Object.defineProperty(navigator, 'webdriver', { get: () => undefined })")

            try:
                print("ğŸŒ æ­£åœ¨åŠ è½½é¡µé¢ (è®¾ç½®30ç§’è¶…æ—¶)...")
                page.goto(url, wait_until="domcontentloaded", timeout=30000)
            except Exception as e:
                print(f"âš ï¸ é¡µé¢åŠ è½½æç¤º (Timeout)ï¼Œæ­£åœ¨åœæ­¢ç½‘é¡µè½¬åœˆä»¥æå–æ•°æ®...")
                try:
                    page.evaluate("window.stop()")
                except: pass

            print("â³ ç¼“å†² 5 ç§’ä»¥ç¡®ä¿äº’åŠ¨æ•°æ®åŠ è½½...")
            time.sleep(5)
            
            try:
                page.mouse.wheel(0, 500)
            except: pass
            time.sleep(2)
            
            # --- æ•°æ®æŠ“å–å‡çº§ï¼šèµè—è¯„æ ¸å¿ƒæå– ---
            stats = {'likes': '0', 'collects': '0', 'comments': '0'}
            try:
                # 1. å°è¯•ç›´æ¥è·å–é€‰æ‹©å™¨ä¸­çš„è®¡æ•°
                # å°çº¢ä¹¦PCç«¯äº’åŠ¨çš„å¸¸è§ç±»åæ˜¯ .count æˆ– .interact-container ä¸­çš„ç‰¹å®š span
                counts = page.query_selector_all(".interact-container .count")
                if len(counts) >= 3:
                    stats['likes'] = counts[0].inner_text()
                    stats['collects'] = counts[1].inner_text()
                    stats['comments'] = counts[2].inner_text()
                else:
                    # 2. æ­£åˆ™å…œåº•é€»è¾‘
                    content = page.content()
                    # åŒ¹é…ï¼šèµ 1.2ä¸‡ æˆ– ç‚¹èµ 1234
                    likes_match = re.search(r'(?:ç‚¹èµ|èµ)\s*([\d\.wä¸‡k]+)', content)
                    collects_match = re.search(r'(?:æ”¶è—|è—)\s*([\d\.wä¸‡k]+)', content)
                    comments_match = re.search(r'(?:è¯„è®º|è¯„)\s*([\d\.wä¸‡k]+)', content)
                    
                    if likes_match: stats['likes'] = likes_match.group(1)
                    if collects_match: stats['collects'] = collects_match.group(1)
                    if comments_match: stats['comments'] = comments_match.group(1)
                
                print(f"ğŸ“Š æŠ“å–åˆ°æ•°æ®ï¼šèµ({stats['likes']}) è—({stats['collects']}) è¯„({stats['comments']})")
            except Exception as e:
                print(f"âš ï¸ æŠ“å–äº’åŠ¨æ•°æ®å¾®ç‘•: {e}")

            final_download_url = real_video_url["url"]

            if not final_download_url:
                print("âš ï¸ æœªç›‘å¬åˆ°ç½‘ç»œæµï¼Œå°è¯•ä»æºç æå–...")
                try:
                    content = page.content()
                    matches = re.findall(r'"masterUrl":"(http[^"]+)"', content)
                    if matches:
                        final_download_url = matches[0].encode('utf-8').decode('unicode_escape')
                        print(f"ğŸ” æºç æå–æˆåŠŸ: {final_download_url[:40]}...")
                except Exception as e:
                    print(f"âš ï¸ æºç æå–å¤±è´¥: {e}")

            if not final_download_url:
                print("âš ï¸ æºç æå–ä¹Ÿå¤±è´¥ï¼Œå°è¯•è·å–æ ‡ç­¾ src...")
                try:
                    video_element = page.query_selector('video')
                    if video_element:
                        src = video_element.get_attribute("src")
                        if src and not src.startswith("blob:"):
                            final_download_url = src
                except: pass

            if not final_download_url:
                raise Exception("âŒ æœªèƒ½æ‰¾åˆ°æœ‰æ•ˆçš„è§†é¢‘åœ°å€")

            # --- å…¶ä»–å…ƒæ•°æ® ---
            title = page.title()
            desc = ""
            try:
                desc_element = page.query_selector("#detail-desc")
                if desc_element: desc = desc_element.inner_text()
            except: pass
            
            author = "Unknown"
            try:
                author_elem = page.query_selector(".username")
                if author_elem: author = author_elem.inner_text().strip()
            except: pass

            author = author.replace("å…³æ³¨", "").strip()

            comments_list = []
            try:
                comment_elements = page.query_selector_all(".comment-item .content")
                for el in comment_elements[:5]: 
                    comments_list.append(el.inner_text())
            except: pass
            
            cover_url = ""
            try:
                meta_img = page.query_selector('meta[property="og:image"]')
                if meta_img: cover_url = meta_img.get_attribute("content")
            except: pass

            video_filename = f"video_{timestamp}.mp4"
            print(f"ğŸ“¥ [Video] å‡†å¤‡ä¸‹è½½...")
            local_video_path = download_video(final_download_url, video_filename)
            
            if not local_video_path:
                raise Exception("è§†é¢‘ä¸‹è½½æœ€ç»ˆå¤±è´¥")

            meta_data = {
                "id": note_id,
                "url": url,
                "title": title,
                "author": author,
                "desc": desc,
                "stats": stats,
                "top_comments": "\n".join(comments_list),
                "cover_url": cover_url,
                "local_video_path": local_video_path,
                "timestamp": timestamp
            }
            
            json_filename = f"meta_{timestamp}.json"
            json_path = os.path.join(WORK_DIR, json_filename)
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(meta_data, f, ensure_ascii=False, indent=2)
                
            print("âœ… [Step 1 å®Œæˆ] æ•°æ®å·²ä¿å­˜")
            time.sleep(1)
            return json_path
        finally:
            if context:
                try:
                    context.close()
                except:
                    pass
